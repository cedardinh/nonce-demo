# 阶段 1：基础设施增强（分布式协调能力）

> 目标：把“单节点可用的 nonce 分配器”推进到“**多节点部署仍然正确**”，并在同一套正确性机制之上支持两种可配置的运行模式：**基础模式** 与 **Worker 队列模式**。

---

## 概要（TL;DR）

这阶段我们只回答一个问题：**多节点同时处理同一个 signer 时，如何保证永远不会重复分配/旧主覆盖新主？**

结论是：

- **正确性基线（所有模式共用）**：任何会写 `signer_nonce_*`（以及后续交易表）的操作，都必须满足：
  1) 先拿到该 signer 的 **租约（lease）**：同一时刻只有一个节点是 owner  
  2) 写入必须携带单调递增的 **fencing token**：数据库拒绝旧 token 写入
- **基础模式（Basic Mode）**：不假设“同 signer 请求会打到同节点”。哪怕请求随机打到任意节点，系统也能正确处理（只是会有更多 lease 争抢/失败重试）。
- **Worker 队列模式（Worker-Queue Mode）**：尽量让同 signer 请求汇聚到同一 worker 节点，并在 worker 内排队串行处理，显著降低 lease 争抢与数据库抖动；但正确性仍然由 lease+fencing 兜底，路由错了最多多一次重试/转发，不会出错。

---

## 目录（读这篇文档的路径）

- **1~2**：把问题“形式化”成不变量（写之前先把正确性说清楚）
- **3~5**：从失败场景推导为什么必须做 lease + fencing（而不是只靠 Redis 锁/随机路由）
- **6~7**：数据模型与核心算法（怎么 acquire/renew、怎么 fenced write）
- **8**：两种运行模式如何共用同一正确性内核（差异只在入口/调度）
- **9**：结合现有代码的最小实现步骤（改哪些类/SQL/配置）
- **10~12**：可观测性、灰度发布、验收测试

---

## 1. 问题定义（把业务变成约束）

我们提供一个能力：对每个 `signer` 分配 nonce（从 0 开始递增，空洞可复用），并提供回写接口 `markUsed/markRecyclable`（语义分别为 `CONSUMED/RELEASED`）。

在单节点里，只要保证“同 signer 串行”就足够。但多节点部署后，会遇到这类真实世界问题：

- 节点 A 卡顿（长 GC/暂停/网络抖动）导致“看起来还活着”，但其实失去了写的资格
- 节点 B 接管后继续推进 nonce
- 节点 A 恢复后仍尝试写入（脑裂/旧主回归）

因此我们要解决的不是“怎么加锁”，而是：

> **如何让任何旧主/失格节点的写入在数据库层面必然失败？**

---

## 2. 正确性不变量（Invariants）

为了让后续设计不跑偏，我们先把必须满足的不变量写出来：

- **I1：单主不变量（Single Owner）**  
  对同一个 signer，在任意时间窗口内最多只有一个节点可以执行“会导致状态推进”的写入。

- **I2：旧主写入隔离（Fencing）**  
  如果发生故障转移（owner 从 A 切到 B），那么 A 在恢复后对该 signer 的所有关键写入都必须被拒绝（不能覆盖 B）。

- **I3：幂等与重试友好**  
  网络抖动/超时导致的重复请求，不应破坏状态（允许同 token 的幂等写入，或通过 request id 等上层机制兜底）。

- **I4：不依赖请求粘性**  
  在基础模式下，不要求“同 signer 的请求一定命中同节点”；系统必须在完全随机路由的情况下仍然正确。

---

## 3. 为什么“仅靠 Redis 分布式锁”不够

即使你有 Redis 锁，也无法从原理上保证 I2（旧主写入隔离）：

- 锁有 TTL，会过期；过期后被新节点获取
- 旧节点可能在锁过期后继续执行并写数据库（它并不知道锁已经丢了）
- 更关键的是：**最终事实在 DB**，如果 DB 接受了旧节点写入，系统就错了

因此：Redis 锁最多是“减少并发”的性能/压力手段，不是多节点正确性的根。

> 本阶段的设计刻意做到：**不需要 Redis 锁也能正确**。Redis 只允许作为可选优化（以后再讨论）。

---

## 4. 从失败场景推导解法：lease + fencing

要满足 I1/I2，最稳的办法是把“谁能写”与“旧主写入隔离”都交给 DB 做最终裁决：

1. **用 DB 租约（lease）回答“谁能写”（I1）**  
   为每个 signer 维护一行 `signer_lease`，字段包含 `owner_node` 与 `expires_at`。

2. **用 fencing token 回答“旧主能不能写”（I2）**  
   当 lease 发生抢占（owner 切换）时，`fencing_token` 单调递增。  
   任何关键写入都带上 token，DB 只接受 **新 token ≥ 旧 token** 的写入。

这样即使节点 A “自以为还能写”，只要它的 token 旧，DB 就会拒绝——从机制上封死脑裂覆盖。

---

## 5. 两种运行模式的关系（先讲清融合逻辑）

这一点很关键：**基础模式**与**Worker 队列模式**不是两套实现。

- 两者共用同一套正确性内核：**lease + fencing + 条件写入**
- 差异只发生在“请求如何到达执行者/如何调度”：
  - 基础模式：不做前置路由，任意节点尝试 acquire，失败就让调用方重试（或网关重试）
  - Worker 队列模式：入口层先把同 signer 的请求尽量汇聚到同一 worker 节点，并在节点内排队串行执行（减少争抢）

> 换句话说：Worker 队列模式是“在正确性内核外面加一层调度/路由”，让系统更快，而不是让系统“才能正确”。

---

## 6. 数据模型（DB 是协调真相）

### 6.1 `signer_lease`（新增）

```sql
CREATE TABLE signer_lease (
  signer        VARCHAR(128) PRIMARY KEY,
  owner_node    VARCHAR(256) NOT NULL,
  fencing_token BIGINT NOT NULL,
  acquired_at   TIMESTAMP NOT NULL,
  expires_at    TIMESTAMP NOT NULL,
  updated_at    TIMESTAMP NOT NULL
);

CREATE INDEX idx_signer_lease_expires_at ON signer_lease(expires_at);
CREATE INDEX idx_signer_lease_owner_node ON signer_lease(owner_node);
```

字段要点：

- `fencing_token`：只在 **抢占** 时 +1；同 owner 续租不变
- `expires_at`：过期判断以 DB 的 `NOW()` 为准（避免节点时钟漂移）

### 6.2 `signer_nonce_allocation`（扩展字段）

```sql
ALTER TABLE signer_nonce_allocation
  ADD COLUMN fencing_token BIGINT NOT NULL DEFAULT 0;

CREATE INDEX idx_signer_nonce_allocation_signer_token
  ON signer_nonce_allocation(signer, fencing_token);
```

> 后续如果引入交易事实表（如 `managed_transaction`），同样要带 `fencing_token`，否则 I2 仍可能被旧主覆盖。

---

## 7. 核心算法（程序员视角一步步落地）

### 7.1 LeaseManager：acquire/renew 的“单 SQL 原子语义”

接口形态：

```java
public interface SignerLeaseManager {
  Optional<Lease> acquireOrRenew(String signer, String nodeId, Duration leaseDuration);
  void release(String signer, long fencingToken);
}
```

关键点：acquire/renew 必须是**原子**的，且抢占必须递增 token。

```sql
INSERT INTO signer_lease (signer, owner_node, fencing_token, acquired_at, expires_at, updated_at)
VALUES (:signer, :nodeId, 1, NOW(), :expiresAt, NOW())
ON CONFLICT (signer) DO UPDATE SET
  owner_node = CASE
    WHEN signer_lease.expires_at < NOW() THEN :nodeId
    WHEN signer_lease.owner_node = :nodeId THEN :nodeId
    ELSE signer_lease.owner_node
  END,
  fencing_token = CASE
    WHEN signer_lease.expires_at < NOW() THEN signer_lease.fencing_token + 1
    ELSE signer_lease.fencing_token
  END,
  acquired_at = CASE
    WHEN signer_lease.expires_at < NOW() OR signer_lease.owner_node = :nodeId THEN NOW()
    ELSE signer_lease.acquired_at
  END,
  expires_at = CASE
    WHEN signer_lease.expires_at < NOW() OR signer_lease.owner_node = :nodeId THEN :expiresAt
    ELSE signer_lease.expires_at
  END,
  updated_at = NOW()
RETURNING signer, owner_node, fencing_token, acquired_at, expires_at;
```

返回判定（在代码里做）：

- 返回行 `owner_node == nodeId` 且 `expires_at > NOW()` ⇒ 成功，得到 token
- 否则 ⇒ 失败（本节点不是 owner）

参数建议：

- `leaseDuration`：10s~30s（看 DB 延迟与 JVM 停顿风险）
- `renewInterval`：`leaseDuration / 3`

### 7.2 Fencing：所有关键写入都必须“带 token + 条件更新”

思路很简单：把“我是谁、我凭什么写”显式带入 SQL，让 DB 作为裁判。

**reserveNonce（HELD）**

```sql
INSERT INTO signer_nonce_allocation
  (signer, nonce, status, locked_until, fencing_token, created_at, updated_at)
VALUES
  (:signer, :nonce, 'HELD', :lockedUntil, :fencingToken, NOW(), NOW())
ON CONFLICT (signer, nonce) DO UPDATE SET
  status        = EXCLUDED.status,
  locked_until  = EXCLUDED.locked_until,
  fencing_token = EXCLUDED.fencing_token,
  updated_at    = NOW()
WHERE
  signer_nonce_allocation.fencing_token <= EXCLUDED.fencing_token
  AND signer_nonce_allocation.status IN ('HELD', 'RELEASED');
```

**markUsed / markRecyclable（语义：CONSUMED / RELEASED）**

```sql
UPDATE signer_nonce_allocation
SET status='CONSUMED', tx_hash=:txHash, updated_at=NOW()
WHERE signer=:signer AND nonce=:nonce AND fencing_token <= :fencingToken;
```

```sql
UPDATE signer_nonce_allocation
SET status='RELEASED', updated_at=NOW()
WHERE signer=:signer AND nonce=:nonce AND fencing_token <= :fencingToken;
```

**fenced reject 的统一处理**：

- 如果更新行数为 0：视为“被 fencing 拒绝”（不是业务错误，是正确性保护触发）
- 行为：记录日志/指标，然后返回可重试错误（基础模式重试；Worker 队列模式重投递/转发到 owner）

---

## 8. 两种模式的运行时流程（把“思维过程”落到时序）

### 8.1 基础模式（Basic Mode）：随机路由也正确

这就是最可靠、最少依赖的形态：

1. 任意节点收到请求
2. 尝试 `acquireOrRenew(signer,nodeId)`
3. 成功：拿到 `fencingToken`，继续分配/回写（写入都带 token）
4. 失败：返回“可重试”（或网关/客户端重试）

它满足 I4，但代价是：

- 多节点会频繁争抢 `signer_lease`（失败重试多）
- 性能受“无效尝试”影响

### 8.2 Worker 队列模式（Worker-Queue Mode）：把争抢变成排队

目标：把同 signer 的请求尽量汇聚到同一 worker 节点，让“串行化”发生在 DB 之前，从而：

- lease acquire 成功率更高
- DB 抖动更小
- 吞吐更稳定（队列吸收突发）

典型结构：

- `SignerRouter`：根据 signer 计算目标 worker 节点（rendezvous hash / consistent hash）
- 目标节点内：按 signer 维度进入队列（或按 worker 单队列，带 signer key 分组）
- worker 线程串行处理该 signer 的请求（处理时仍按 7.1/7.2 走 lease+fencing）

> 这本质上是“把随机竞争变为有序排队”，但不改变正确性内核。

**错误路由兜底（两种选一，先从最少实现开始）**

- **客户端重试（推荐先落地）**
  - 非 owner 节点返回 `409/503 + Retry-After`
  - 客户端按同一套路由表重试
- **服务端转发（体验更好，复杂度更高）**
  - 非 owner 节点查 `signer_lease.owner_node` 并转发给 owner

---

## 9. 结合现有项目的最小实现步骤（按提交顺序）

> 原则：先让基础模式在多节点下正确，再加 Worker 队列模式提速。

### Step 0：先把“无 Redis 也能跑”作为默认（已在代码里完成）

- Redis 锁不作为必选依赖，关闭时不装配相关 bean
- `NonceService` 在 redis-disabled 时不依赖 RedisLockManager

### Step 1：数据库迁移（先上表/字段）

- 新增 `signer_lease`
- `signer_nonce_allocation` 增加 `fencing_token`

### Step 2：实现 `SignerLeaseManager`

- 基于 MyBatis / JDBC 写入第 7.1 的 UPSERT
- 产出 `Lease(fencingToken, ownerNode, expiresAt...)`

### Step 3：把 lease+token 串入 `NonceService` 分配路径

在 `allocate/doAllocate` 开头增加：

  - `leaseManager.acquireOrRenew(signer,nodeId,leaseDuration)`
- 拿到 `fencingToken` 后，调用 repository 的 reserve/mark*（带 token）

### Step 4：改造 `NonceRepository` 的写入为 fenced write

新增/替换：

- `reserveNonce(..., fencingToken)`
- `markUsed(..., fencingToken)`
- `markRecyclable(..., fencingToken)`
- 回收过期 reserved 的 SQL（是否需要 fencing，取决于是否允许非 owner 做清理；建议也加 token 或仅 owner 清理）

### Step 5：定义并实现“fenced reject”的统一错误语义

- fenced reject 不算业务失败，是“正确性保护”
- 对外返回可重试错误（基础模式）；队列模式走重投递/转发

### Step 6（可选）：接入 Worker 队列模式（先最小版）

最小可落地版本：

- 一个 `SignerRouter`（哈希到节点）
- 客户端/网关按路由调用
- 失败走客户端重试（不做服务端转发）

---

## 10. 可观测性（最小集合）

- **Lease**：`lease.acquire.success|fail`, `lease.renew.success|fail`
- **Fencing**：`fence.reject.count{op=reserve|markUsed|markRecyclable}`
- **队列模式**（如果开启）：`worker.queue.depth`, `worker.queue.latency`

日志：

- acquire 成功/抢占：`signer, owner_node, fencing_token, expires_at`
- fenced reject：`signer, op, provided_token, nodeId`

---

## 11. 灰度与回滚

推荐顺序：

1. 上 DB 迁移（无业务变更）
2. 上应用代码，但先只启用基础模式（仍按旧入口调用）
3. 开启 lease+fencing（先小流量 signer）
4. 最后再开启 Worker 队列模式（只影响性能，不影响正确性）

回滚策略：

- 关闭 Worker 队列模式 ⇒ 回到基础模式
- 如必须回退到旧逻辑：保留新增表/字段不影响（字段默认值可兜底），但生产不建议长期运行在“无 fencing”的多节点写路径

---

## 12. 验收测试（阶段 1 完成的定义）

**基础模式正确性（必须）**

- 多节点随机打同 signer 请求：分配无重复，状态推进无覆盖
- 故障转移：B 抢占后 token+1，A 恢复写入被拒绝（fenced reject）

**Worker 队列模式性能（可选，但建议）**

- 同 signer 命中同 worker：lease acquire fail 显著下降
- 队列压力下延迟可控（队列深度/等待时间可观测）

---

## 13. 开放问题（实现前必须确认）

- `nodeId` 的构成：`hostname:port` / `podName` / `instanceId`（必须稳定且可追踪）
- fenced write 策略：`<=`（允许同 token 幂等）还是 `<`（更严格但需要额外幂等策略）
- 回收过期 HELD 是否允许“非 owner 清理”？（若允许，清理也必须 fenced；若不允许，则只让 owner 清理）


